BO

	[ ] Both referees worry about the statistical power of your study—R1, for example, notes that you would be unable to statistically detect a 30 percent increase in savings, which seems problematic. This is important for studies that find null reports. 

	[ ] Both R1 and R2 also raise external validity concerns—R1 noting that this isn’t a “real” bank product with the usual features, and R2 noting the unusual sampleand a variety of other unusual features of the account. 

	[ ] Finally, R2 discusses the fact that most savings papers now focus on total savings, not just savings in a particular account, which makes sense from an economics perspective

R1

	[ ] In one PLS treatment arm, participants were assigned lottery tickets even if they did not saved andinformed if they won (or would have won, had they made deposits – I’ll call this PLS-D fordisclosure); in the other arm, lottery numbers and results were only disclosed to individualswho deposited (PLS-N for “no disclosure”).

	[ ] The paper is underpowered: with just over 300 participants spread across three treat-ment arms, the authors don’t have enough power to reject non-trivial shifts in keyoutcomes – especially once a FWER correction is applied. For example the standarderror on “total deposit amount” is around 3 (see Table 3); but the control mean is 14.87– which means that a 30% increase in deposits would not be statistically significant. Generally the paper is clear on this, but you over-interpret a bit at times whencomparing PLS-D and PLS-N (especially in the introduction/when you say regretaversion accounts for 20% of the change in deposit behavior). The paper shouldalways be clear that you cannot reject equality.

	[ ] External validity is limited: the authors built a custom product – it was not offeredby a bank, but rather the Busara Center for Behavioral Economics.  The product only lasted for 2 months (did people know this in advance?) and ran off of transferring airtime to a project phone. Participants were drawn from Busara’s pool of respondents (so presumably they have past experience participating in research studies? Do you know how many had gone through Busara projects before?).  I am not inherently opposed to studies that leverage niche populations or designs, but it would be usefulto see a little more discussion of how you think this might matter – specifically I worry that individuals may have limited deposits because they didn’t trust the product/were worried they wouldn’t get their money back.

	[ ] Indeed, given that (as I understand it), respondents got 5% of a deposit no matterwhen they deposited. This is an extraordinarily high interest rate: you could,theoretically, deposit your life savings in the product on day 29, pull it out onday 30, deposit again on day 59, and take everything out on day 60. Based onthe graphs it didn’t look like many people did this, but if not, why do you thinkthis sort of strategic behavior was limited.

	[ ] The second test of regret aversion is compelling, but I don’t think it rules out analternative explanation, which is that people revise their expectations of being “lucky”up when they are told they had a winning ticket.  In fact you have evidence fromthe endline that beliefs about luck shifted. One way to test this would be to run thesame test for people who deposited on day t-1 (so they actually won/didn’t experienceregret). There would be some income effect bundled in here, but I’d expect that wouldbe small for most given how small deposits are.

	[ ] It would be useful to show the text of the messages people got in an online appendix so the reader can better understand the nature of the treatments.

	[ ] PLS may have been a poor substitute to gambling in your setting because of thecommitment aspect of the product: lottery winners cannot immediately access theirwinnings, which strips out the immediate gratification aspect gambling typically has.

	[ ] You mention a PAP but don’t discuss how your methods relate to it. I would makeclearer in the discussion how you follow/deviate from the PAP.

R2

	[ ] While I think this is a nice study, on the whole I think this study has more of the flavor of a “lab experiment in the field” and perhaps might be a better fit for more of an experimental journal. The main reason I say this is because there have been a huge number of savings papers in the last 10-15 years, and in my opinion the best types of studies these days have large samples and long enough timeframes, and they measure the real impacts of savings on people’s overall lives. In addition, to be policy relevant, papers should focus on financial products that are available to people in real life, and which aren’t particular to the experiment (even if interesting). 

	[ ] A first issue is that the sample is small and the sample isn’t representative. While early papers in the development literature had small samples like this, they also were investing resources into comprehensive surveys; and anyway, by now the size of the stronger experiments have grown by a lot. Earlier papers also tended to focus on a particular group of people, such as small-scale entrepreneurs, or in some cases representative samples of people living near banks. By contrast, the sample here seems more like a convenience sample, and there isn’t any discussion of how these individuals compare to the average person in Kenya.  

	[ ] In cases where the experimental account offers higher interest (like this one), it seems near-certain that this would happen – that people actually should transfer money from some other account to this one that offers 5% daily interest. In many cases, researchers might put in a table showing treatment savings, then other savings (like saving at home or in other places), and then total savings. As far as I understand it, the main results in the main tables is only for the treatment account; and I think the crowd out effects are deep in the appendix (in Appendix Table 11). This seem to show some negative effects on PLS (not significant), which might make sense if people are switching money around to get the high interest rate.

	[ ] I initially thought Table 3 was only savings in the experiment, but then in the intro on p.3 the authors note that they find no change in total savings, and argue that this shows there is no displacement. But if the effect in Table 3 is for experimental savings, then there isn’t any displacement to worry about, since the experiment had no effect on even experimental savings.

	[ ] A third issue is that these accounts weren’t that natural. The interest rate appears to be astronomical – 5% per day. Why did the researchers pick such a high number? But on the flip side, the account is not very natural – my reading is that the account is essentially just a cell phone where people were texting airtime (not mobile money even), and then were getting this money sent back. While I think this was an interesting idea to set this up, I’m not sure how practical this is outside of this context.

	[ ] By the way, a related question I have here is about the legality of holding people’s deposits – my understanding was that holding deposits required certification from regulatory agencies, which is why many MFIs don’t hold deposits. I understand that this was just a small experiment with only 300 people, but could the authors comment on this? I could be mistaken on this point.

	[ ] The effects seem pretty modest. The amount saved is surprisingly low given the stakes, just a few dollars, which might be because of the frictions involved with making these transactions, or trust issues. But the prize linked savings doesn’t have an effect on total savings. And as I mentioned above, my main interest is in whether an intervention like this would have effects beyond savings in any case.  

	[ ] the timeline of 60 days is short, much shorter than other papers in this area.
